from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from datetime import timedelta

default_args = {
    'owner': 'noura',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
    'execution_timeout': timedelta(hours=4),
}

dag = DAG(
    'banking_reviews_complete_pipeline_dbt',
    default_args=default_args,
    description='Complete Banking Reviews Data Warehouse Pipeline with DBT',
    schedule_interval='@daily',
    start_date=days_ago(1),
    catchup=False,
    max_active_runs=1,
    tags=['banking', 'reviews', 'dbt', 'data-warehouse'],
)

# =====================================
# VARIABLES DE CONFIGURATION
# =====================================

# CHEMIN CORRECT DE VOTRE PROJET DBT
DBT_PROJECT_PATH = "/home/noura/dbt_project/morocco_banks_reviews"
VENV_PATH = "/home/noura/venv"
SCRIPTS_PATH = "/home/noura/airflow/scripts"

# =====================================
# PHASE 1: COLLECTE DES DONNÃ‰ES
# =====================================

# TÃ¢che 1: Scraping Google Maps (votre script existant)
run_scraping_script = BashOperator(
    task_id='run_scraping_script',
    bash_command=f'{VENV_PATH}/bin/python {SCRIPTS_PATH}/script1.py',
    dag=dag,
)

# TÃ¢che 2: Insertion en table staging (votre script existant)
insert_data_to_postgres = BashOperator(
    task_id='insert_data_to_postgres',
    bash_command=f'{VENV_PATH}/bin/python {SCRIPTS_PATH}/insert_into_postgres.py',
    dag=dag,
)

# =====================================
# PHASE 2: VÃ‰RIFICATION ET PRÃ‰PARATION DBT
# =====================================

# TÃ¢che 3: VÃ©rification de la configuration DBT
dbt_debug = BashOperator(
    task_id='dbt_debug_check',
    bash_command=f'''
    cd {DBT_PROJECT_PATH} && 
    {VENV_PATH}/bin/dbt debug
    ''',
    dag=dag,
)

# TÃ¢che 4: Installation/mise Ã  jour des packages DBT
dbt_deps = BashOperator(
    task_id='dbt_install_dependencies',
    bash_command=f'''
    cd {DBT_PROJECT_PATH} && 
    {VENV_PATH}/bin/dbt deps
    ''',
    dag=dag,
)

# =====================================
# PHASE 3: TRANSFORMATION DBT - STAGING
# =====================================

# TÃ¢che 5: DBT Staging - Nettoyage initial
dbt_staging = BashOperator(
    task_id='dbt_run_staging',
    bash_command=f'''
    cd {DBT_PROJECT_PATH} && 
    {VENV_PATH}/bin/dbt run --select models/staging --vars '{{"execution_date": "{{{{ ds }}}}"}}' --target dev
    ''',
    dag=dag,
)

# TÃ¢che 6: DBT Intermediate - DÃ©duplication et nettoyage avancÃ©
dbt_intermediate = BashOperator(
    task_id='dbt_run_intermediate',
    bash_command=f'''
    cd {DBT_PROJECT_PATH} && 
    {VENV_PATH}/bin/dbt run --select models/intermediate --vars '{{"execution_date": "{{{{ ds }}}}"}}' --target dev
    ''',
    dag=dag,
)

# =====================================
# PHASE 4: ENRICHISSEMENT NLP
# =====================================

# TÃ¢che 7: Analyse LDA pour extraction des topics
run_lda_analysis = BashOperator(
    task_id='run_lda_topic_analysis',
    bash_command=f'{VENV_PATH}/bin/python {SCRIPTS_PATH}/lda_topic_modeling.py',
    dag=dag,
)

# =====================================
# PHASE 5: MODÃ‰LISATION DIMENSIONNELLE DBT
# =====================================

# TÃ¢che 8: CrÃ©ation de la mart enrichie
dbt_mart_enriched = BashOperator(
    task_id='dbt_run_mart_enriched',
    bash_command=f'''
    cd {DBT_PROJECT_PATH} && 
    {VENV_PATH}/bin/dbt run --select models/marts/mart_reviews_enriched.sql --vars '{{"execution_date": "{{{{ ds }}}}"}}' --target dev
    ''',
    dag=dag,
)

# TÃ¢che 9: Construction des dimensions
dbt_dimensions = BashOperator(
    task_id='dbt_run_dimensions',
    bash_command=f'''
    cd {DBT_PROJECT_PATH} && 
    {VENV_PATH}/bin/dbt run --select models/marts/dim_* --vars '{{"execution_date": "{{{{ ds }}}}"}}' --target dev
    ''',
    dag=dag,
)

# TÃ¢che 10: Construction de la table de faits
dbt_fact_table = BashOperator(
    task_id='dbt_run_fact_table',
    bash_command=f'''
    cd {DBT_PROJECT_PATH} && 
    {VENV_PATH}/bin/dbt run --select models/marts/fact_reviews.sql --vars '{{"execution_date": "{{{{ ds }}}}"}}' --target dev
    ''',
    dag=dag,
)

# =====================================
# PHASE 6: TESTS ET VALIDATION
# =====================================

# TÃ¢che 11: Tests de qualitÃ© DBT
dbt_tests = BashOperator(
    task_id='dbt_run_tests',
    bash_command=f'''
    cd {DBT_PROJECT_PATH} && 
    {VENV_PATH}/bin/dbt test --vars '{{"execution_date": "{{{{ ds }}}}"}}' --target dev
    ''',
    dag=dag,
)

# TÃ¢che 12: GÃ©nÃ©ration de la documentation DBT
dbt_docs = BashOperator(
    task_id='dbt_generate_docs',
    bash_command=f'''
    cd {DBT_PROJECT_PATH} && 
    {VENV_PATH}/bin/dbt docs generate --target dev &&
    echo "ðŸ“š Documentation DBT gÃ©nÃ©rÃ©e dans {DBT_PROJECT_PATH}/target/"
    ''',
    dag=dag,
)

# =====================================
# PHASE 7: OPTIMISATION POUR DASHBOARDS
# =====================================

# TÃ¢che 13: CrÃ©ation des vues matÃ©rialisÃ©es pour Looker Studio
create_dashboard_views = PostgresOperator(
    task_id='create_dashboard_materialized_views',
    postgres_conn_id='postgres_default',  # Utilisez votre connexion Airflow
    sql=f'''
    -- Vue pour tendances sentiment par banque
    DROP MATERIALIZED VIEW IF EXISTS marts.mv_sentiment_trends_by_bank CASCADE;
    CREATE MATERIALIZED VIEW marts.mv_sentiment_trends_by_bank AS
    SELECT 
        db.bank_name,
        ds.sentiment,
        DATE_TRUNC('month', TO_DATE(fr.date_avis, 'DD/MM/YYYY')) as month_year,
        COUNT(*) as review_count,
        AVG(fr.rating) as avg_rating,
        ROUND(
            COUNT(CASE WHEN ds.sentiment = 'Positif' THEN 1 END)::DECIMAL / 
            NULLIF(COUNT(*), 0) * 100, 2
        ) as positive_percentage
    FROM marts.fact_reviews fr
    JOIN marts.dim_bank db ON fr.bank_key = db.bank_key
    JOIN marts.dim_sentiment ds ON fr.sentiment_key = ds.sentiment_key
    WHERE fr.review_date IS NOT NULL
    GROUP BY db.bank_name, ds.sentiment, DATE_TRUNC('month', TO_DATE(fr.date_avis, 'DD/MM/YYYY'));
    
    -- Vue pour performance des agences par ville
    DROP MATERIALIZED VIEW IF EXISTS marts.mv_city_bank_performance CASCADE;
    CREATE MATERIALIZED VIEW marts.mv_city_bank_performance AS
    SELECT 
        dl.city,
        db.bank_name,
        COUNT(*) as total_reviews,
        AVG(fr.rating) as avg_rating,
        COUNT(CASE WHEN ds.sentiment = 'Positif' THEN 1 END) as positive_reviews,
        COUNT(CASE WHEN ds.sentiment = 'Negatif' THEN 1 END) as negative_reviews,
        ROUND(
            COUNT(CASE WHEN ds.sentiment = 'Positif' THEN 1 END)::DECIMAL / 
            NULLIF(COUNT(*), 0) * 100, 2
        ) as satisfaction_rate
    FROM marts.fact_reviews fr
    JOIN marts.dim_bank db ON fr.bank_key = db.bank_key
    JOIN marts.dim_location dl ON fr.location_key = dl.location_key
    JOIN marts.dim_sentiment ds ON fr.sentiment_key = ds.sentiment_key
    GROUP BY dl.city, db.bank_name;
    
    -- Vue pour analyse des topics
    DROP MATERIALIZED VIEW IF EXISTS marts.mv_topic_insights CASCADE;
    CREATE MATERIALIZED VIEW marts.mv_topic_insights AS
    SELECT 
        dt.topic_name,
        dt.category,
        COUNT(*) as mentions_count,
        AVG(fr.rating) as avg_rating_for_topic,
        COUNT(CASE WHEN ds.sentiment = 'Positif' THEN 1 END) as positive_mentions,
        COUNT(CASE WHEN ds.sentiment = 'Negatif' THEN 1 END) as negative_mentions,
        STRING_AGG(DISTINCT db.bank_name, ', ' ORDER BY db.bank_name) as banks_mentioned
    FROM marts.fact_reviews fr
    JOIN marts.dim_topic dt ON fr.topic_key = dt.topic_key
    JOIN marts.dim_sentiment ds ON fr.sentiment_key = ds.sentiment_key
    JOIN marts.dim_bank db ON fr.bank_key = db.bank_key
    GROUP BY dt.topic_name, dt.category;
    
    -- Actualiser les statistiques
    ANALYZE marts.mv_sentiment_trends_by_bank;
    ANALYZE marts.mv_city_bank_performance;
    ANALYZE marts.mv_topic_insights;
    
    -- Log de succÃ¨s
    INSERT INTO public.pipeline_logs (execution_date, step_name, status, message)
    VALUES (CURRENT_DATE, 'create_dashboard_views', 'SUCCESS', 'Vues matÃ©rialisÃ©es crÃ©Ã©es pour Looker Studio');
    ''',
    dag=dag,
)

# =====================================
# PHASE 8: VALIDATION ET MONITORING
# =====================================

def validate_pipeline_data(**context):
    """Validation des donnÃ©es du pipeline"""
    import psycopg2
    import pandas as pd
    
    try:
        conn = psycopg2.connect(
            host="localhost",
            database="bank_maroc",
            user="airflow1",
            password="airflow"
        )
        
        # VÃ©rifications critiques
        checks = {
            'total_reviews': "SELECT COUNT(*) as count FROM marts.fact_reviews",
            'banks_count': "SELECT COUNT(DISTINCT bank_key) as count FROM marts.fact_reviews WHERE bank_key > 0",
            'recent_data': "SELECT COUNT(*) as count FROM marts.mart_reviews_enriched WHERE date_avis != 'Date inconnue'",
            'sentiment_distribution': """
                SELECT sentiment, COUNT(*) as count 
                FROM marts.fact_reviews fr 
                JOIN marts.dim_sentiment ds ON fr.sentiment_key = ds.sentiment_key 
                GROUP BY sentiment
            """
        }
        
        results = {}
        for check_name, query in checks.items():
            df = pd.read_sql(query, conn)
            results[check_name] = df.to_dict('records')
            print(f"âœ… {check_name}: {df.to_dict('records')}")
        
        conn.close()
        
        # Validations mÃ©tier
        total_reviews = results['total_reviews'][0]['count']
        banks_count = results['banks_count'][0]['count']
        recent_data = results['recent_data'][0]['count']
        
        if total_reviews < 50:
            raise ValueError(f"âŒ Nombre total d'avis insuffisant: {total_reviews}")
        
        if banks_count < 3:
            raise ValueError(f"âŒ Nombre de banques insuffisant: {banks_count}")
        
        if recent_data < 10:
            print(f"âš ï¸  Warning: Peu de donnÃ©es avec dates valides: {recent_data}")
        
        print(f"ðŸŽ‰ VALIDATION RÃ‰USSIE!")
        print(f"ðŸ“Š Total avis: {total_reviews}")
        print(f"ðŸ¦ Banques: {banks_count}")
        print(f"ðŸ“… DonnÃ©es datÃ©es: {recent_data}")
        
        return results
        
    except Exception as e:
        print(f"âŒ ERREUR lors de la validation: {e}")
        raise

# TÃ¢che 14: Validation des donnÃ©es
validate_data_quality = PythonOperator(
    task_id='validate_pipeline_data_quality',
    python_callable=validate_pipeline_data,
    dag=dag,
)

# TÃ¢che 15: Notification de fin
send_success_notification = BashOperator(
    task_id='send_pipeline_success_notification',
    bash_command=f'''
    echo "ðŸŽ‰ PIPELINE DATA WAREHOUSE TERMINÃ‰ AVEC SUCCÃˆS!"
    echo "================================================"
    echo "ðŸ“… Date d'exÃ©cution: {{{{ ds }}}}"
    echo "ðŸ“ Projet DBT: {DBT_PROJECT_PATH}"
    echo "ðŸ¦ DonnÃ©es disponibles dans le schÃ©ma 'marts'"
    echo "ðŸ“ˆ Vues matÃ©rialisÃ©es crÃ©Ã©es pour Looker Studio:"
    echo "   - marts.mv_sentiment_trends_by_bank"
    echo "   - marts.mv_city_bank_performance" 
    echo "   - marts.mv_topic_insights"
    echo "ðŸ“š Documentation DBT: {DBT_PROJECT_PATH}/target/index.html"
    echo ""
    echo "ðŸ”— Connexion PostgreSQL pour Looker Studio:"
    echo "   Host: localhost"
    echo "   Database: bank_maroc"
    echo "   Schema: marts"
    echo ""
    echo "âœ… Pipeline prÃªt pour l'analyse business intelligence!"
    ''',
    dag=dag,
)

# =====================================
# DÃ‰FINITION DES DÃ‰PENDANCES
# =====================================

# Phase 1: Collecte des donnÃ©es
run_scraping_script >> insert_data_to_postgres

# Phase 2: PrÃ©paration DBT
insert_data_to_postgres >> dbt_debug >> dbt_deps

# Phase 3: Transformations DBT Staging/Intermediate
dbt_deps >> dbt_staging >> dbt_intermediate

# Phase 4: Enrichissement NLP
dbt_intermediate >> run_lda_analysis

# Phase 5: ModÃ©lisation dimensionnelle
run_lda_analysis >> dbt_mart_enriched >> dbt_dimensions >> dbt_fact_table

# Phase 6: Tests et documentation
dbt_fact_table >> dbt_tests >> dbt_docs

# Phase 7: Optimisation dashboards
dbt_docs >> create_dashboard_views

# Phase 8: Validation et notification
create_dashboard_views >> validate_data_quality >> send_success_notification